# Unpaired Semantic Image Synthesis with SPADE
TODO
<div align="center">
  <img src="docs/images/ocean.gif" height="200"/>
  <img src="docs/images/treepond.gif" height="200"/>
</div>


### Model Architecture

<div align="center">
  <img src="docs/images/new_model.png" height="250"/>
</div>
TODO

## Experiment Results 
TODO

## Installation

Clone this repo.
```bash
git clone https://github.com/NVlabs/SPADE.git
cd SPADE/
```

This code requires PyTorch 1.0 and python 3+. Please install dependencies by
```bash
pip install -r requirements.txt
```


## Dataset Preparation

For COCO-Stuff, Cityscapes or ADE20K, the datasets can be downloaded and prepared following [SPADE](https://github.com/NVlabs/SPADE.git). 

The ADE20K dataset used to test the model  can be downloaded [here](http://data.csail.mit.edu/places/ADEchallenge/ADEChallengeData2016.zip), which is from [MIT Scene Parsing BenchMark](http://sceneparsing.csail.mit.edu/). After unzipping it, put the jpg image files `ADEChallengeData2016/images/` and png label files `ADEChallengeData2016/annotatoins/` inside the `datasets/` directory. 


## Generating Images Using a Pre-trained Model

Once the dataset is ready, the result images can be generated using pre-trained models:

1. Download the zip of the pre-trained models from [here (TODO)](), unzip it and save it in 'checkpoints/'.

2. Generate images using the pre-trained model.
    ```bash
    python test.py --name [type]_pretrained --dataset_mode [dataset] --dataroot [path_to_dataset]
    ```
    `[type]_pretrained` is the directory name of the checkpoint file downloaded in Step 1, which should be one of `coco_pretrained`, `ade20k_pretrained`, and `cityscapes_pretrained`. `[dataset]` can be one of `coco`, `ade20k`, and `cityscapes`, and `[path_to_dataset]`, is the path to the dataset. If you are running on CPU mode, append `--gpu_ids -1`.

3. The outputs images are stored at `./results/[type]_pretrained/` by default. You can view them using the autogenerated HTML file in the directory.


## Training New Models

New models can be trained with the following commands.


```bash
python train.py --name [experiment_name] --dataset_mode [dataset_mode] --dataroot [path_to_ade20k_dataset]
```

There are many options you can specify. Please use `python train.py --help`. The specified options are printed to the console.

## Testing Pre-trained Models

Testing is similar to generating images using pre-trained models.

```bash
python test.py --name [name_of_experiment] --dataset_mode [dataset_mode] --dataroot [path_to_dataset]
```

Use `--results_dir` to specify the output directory. `--how_many` will specify the maximum number of images to generate. By default, it loads the latest checkpoint. It can be changed using `--which_epoch`.

Use `--eval` to compute FID score, mIoU and accuracy over the generated images.

## Available Options
Besides the options added in the original [SPADE](https://github.com/NVlabs/SPADE.git) repository, new options have been added:
- TODO

## Code Structure

- `train.py`, `test.py`: the entry point for training and testing.
- `trainers/pix2pix_trainer.py`: harnesses and reports the progress of training.
- `models/pix2pix_model.py`: creates the networks, and compute the losses
- `models/networks/`: defines the architecture of all models
- `options/`: creates option lists using `argparse` package. More individuals are dynamically added in other files as well. Please see the section below.
- `data/`: defines the class for loading images and label maps.


## Acknowledgments
This code borrows heavily from [SPADE](https://github.com/NVlabs/SPADE.git) and [CSAILVision's Semantic Segmentation](https://github.com/CSAILVision/semantic-segmentation-pytorch).